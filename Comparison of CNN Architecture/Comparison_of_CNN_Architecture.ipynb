{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPRr3g3tF5lP"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    # Load datasets\n",
        "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "    (x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
        "    (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "\n",
        "    # Normalize the data to the range [0, 1]\n",
        "    x_train_mnist, x_test_mnist = x_train_mnist / 255.0, x_test_mnist / 255.0\n",
        "    x_train_fmnist, x_test_fmnist = x_train_fmnist / 255.0, x_test_fmnist / 255.0\n",
        "    x_train_cifar, x_test_cifar = x_train_cifar / 255.0, x_test_cifar / 255.0\n",
        "\n",
        "    # Reshape the data to add a channel dimension\n",
        "    x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
        "    x_test_mnist = x_test_mnist.reshape(-1, 28, 28, 1)\n",
        "    x_train_fmnist = x_train_fmnist.reshape(-1, 28, 28, 1)\n",
        "    x_test_fmnist = x_test_fmnist.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    y_train_mnist = to_categorical(y_train_mnist)\n",
        "    y_test_mnist = to_categorical(y_test_mnist)\n",
        "    y_train_fmnist = to_categorical(y_train_fmnist)\n",
        "    y_test_fmnist = to_categorical(y_test_fmnist)\n",
        "    y_train_cifar = to_categorical(y_train_cifar)\n",
        "    y_test_cifar = to_categorical(y_test_cifar)\n",
        "\n",
        "    return (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist), \\\n",
        "           (x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist), \\\n",
        "           (x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)\n",
        "\n",
        "# Load and preprocess the data\n",
        "mnist_data, fmnist_data, cifar_data = load_and_preprocess_data()\n",
        "\n",
        "# Extract preprocessed data\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = mnist_data\n",
        "x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist = fmnist_data\n",
        "x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar = cifar_data\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"MNIST:\", x_train_mnist.shape, y_train_mnist.shape, x_test_mnist.shape, y_test_mnist.shape)\n",
        "print(\"Fashion MNIST:\", x_train_fmnist.shape, y_train_fmnist.shape, x_test_fmnist.shape, y_test_fmnist.shape)\n",
        "print(\"CIFAR-10:\", x_train_cifar.shape, y_train_cifar.shape, x_test_cifar.shape, y_test_cifar.shape)"
      ],
      "metadata": {
        "id": "Jjkdk7DgG8Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Alexnet.py**"
      ],
      "metadata": {
        "id": "-ecYEL_gIJcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to build AlexNet model with adjusted pooling layers\n",
        "def build_alexnet(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Layer 1\n",
        "    model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), input_shape=input_shape, padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Layer 2\n",
        "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Layer 3\n",
        "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "    # Layer 4\n",
        "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "    # Layer 5\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Flatten and Fully Connected Layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to preprocess and load MNIST data\n",
        "def load_and_preprocess_mnist():\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "\n",
        "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "    # Normalize data\n",
        "    x_train_mnist = np.expand_dims(x_train_mnist, axis=-1).astype('float32') / 255.0\n",
        "    x_test_mnist = np.expand_dims(x_test_mnist, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    # Resize images to 32x32 to fit AlexNet input size\n",
        "    x_train_mnist = np.pad(x_train_mnist, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test_mnist = np.pad(x_test_mnist, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "\n",
        "# Function to preprocess and load Fashion MNIST data\n",
        "def load_and_preprocess_fashion_mnist():\n",
        "    from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "    (x_train_fashion_mnist, y_train_fashion_mnist), (x_test_fashion_mnist, y_test_fashion_mnist) = fashion_mnist.load_data()\n",
        "\n",
        "    # Normalize data\n",
        "    x_train_fashion_mnist = np.expand_dims(x_train_fashion_mnist, axis=-1).astype('float32') / 255.0\n",
        "    x_test_fashion_mnist = np.expand_dims(x_test_fashion_mnist, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    # Resize images to 32x32 to fit AlexNet input size\n",
        "    x_train_fashion_mnist = np.pad(x_train_fashion_mnist, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test_fashion_mnist = np.pad(x_test_fashion_mnist, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "\n",
        "# Function to preprocess and load CIFAR-10 data\n",
        "def load_and_preprocess_cifar10():\n",
        "    from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "    (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "\n",
        "    # Normalize data\n",
        "    x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "    x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "    return (x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)\n",
        "\n",
        "# Function to train AlexNet on a dataset\n",
        "def train_alexnet(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    # Build AlexNet model\n",
        "    model = build_alexnet(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training AlexNet on {dataset_name} dataset...\")\n",
        "    history = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128, validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Evaluating AlexNet on {dataset_name} dataset...\")\n",
        "    test_loss, test_acc = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "    precision = precision_score(y_test, y_pred_class, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_class, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
        "\n",
        "    # Print and save evaluation metrics to a text file\n",
        "    metrics_text = f\"Dataset: {dataset_name}\\n\" \\\n",
        "                   f\"Test accuracy: {test_acc:.4f}\\n\" \\\n",
        "                   f\"Precision: {precision:.4f}\\n\" \\\n",
        "                   f\"Recall: {recall:.4f}\\n\" \\\n",
        "                   f\"F1-score: {f1:.4f}\\n\"\n",
        "\n",
        "    with open(f\"{dataset_name}_alexnet_metrics.txt\", 'w') as f:\n",
        "        f.write(metrics_text)\n",
        "\n",
        "    print(metrics_text)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{dataset_name} Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{dataset_name} Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig(f\"{dataset_name}_alexnet_training_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Load and preprocess datasets\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_and_preprocess_mnist()\n",
        "x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist = load_and_preprocess_fashion_mnist()\n",
        "x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar = load_and_preprocess_cifar10()\n",
        "\n",
        "# Train and evaluate AlexNet on MNIST\n",
        "train_alexnet('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "\n",
        "# Train and evaluate AlexNet on Fashion MNIST\n",
        "train_alexnet('Fashion_MNIST', x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "\n",
        "# Train and evaluate AlexNet on CIFAR-10\n",
        "train_alexnet('CIFAR-10', x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)"
      ],
      "metadata": {
        "id": "KARGOuKaIY_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LeNet-5.py**"
      ],
      "metadata": {
        "id": "YPyhKM7iIyLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to define and train LeNet-5 on a specific dataset\n",
        "def train_lenet5(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    print(f\"\\nStarting training on {dataset_name} dataset...\")\n",
        "    # Define LeNet-5 model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=x_train.shape[1:]))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(120, activation='tanh'))\n",
        "    model.add(layers.Dense(84, activation='tanh'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    print(f\"\\nTraining LeNet-5 on {dataset_name} dataset:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Starting training on {dataset_name} dataset...\")\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=128,\n",
        "                        epochs=20,\n",
        "                        verbose=1,\n",
        "                        validation_data=(x_test, y_test))\n",
        "    print(f\"Completed training on {dataset_name} dataset.\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f'Test accuracy on {dataset_name}: {test_acc}')\n",
        "\n",
        "    # Predict the classes for the test set\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = tf.argmax(y_pred, axis=1).numpy()\n",
        "    y_test_classes = tf.argmax(y_test, axis=1).numpy()\n",
        "\n",
        "    # Calculate precision, recall, F1-score\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'Precision on {dataset_name}: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss/Accuracy')\n",
        "    plt.title(f'Training and Validation Loss and Accuracy - {dataset_name}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return test_acc, precision, recall, f1_score\n",
        "\n",
        "# Load and preprocess datasets\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "x_train_mnist, x_test_mnist = x_train_mnist / 255.0, x_test_mnist / 255.0\n",
        "x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
        "x_test_mnist = x_test_mnist.reshape(-1, 28, 28, 1)\n",
        "y_train_mnist = to_categorical(y_train_mnist, num_classes=10)\n",
        "y_test_mnist = to_categorical(y_test_mnist, num_classes=10)\n",
        "\n",
        "(x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
        "x_train_fmnist, x_test_fmnist = x_train_fmnist / 255.0, x_test_fmnist / 255.0\n",
        "x_train_fmnist = x_train_fmnist.reshape(-1, 28, 28, 1)\n",
        "x_test_fmnist = x_test_fmnist.reshape(-1, 28, 28, 1)\n",
        "y_train_fmnist = to_categorical(y_train_fmnist, num_classes=10)\n",
        "y_test_fmnist = to_categorical(y_test_fmnist, num_classes=10)\n",
        "\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "x_train_cifar, x_test_cifar = x_train_cifar / 255.0, x_test_cifar / 255.0\n",
        "y_train_cifar = to_categorical(y_train_cifar, num_classes=10)\n",
        "y_test_cifar = to_categorical(y_test_cifar, num_classes=10)\n",
        "\n",
        "# Train and evaluate LeNet-5 on each dataset\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate on MNIST dataset\n",
        "acc_mnist, prec_mnist, rec_mnist, f1_mnist = train_lenet5('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "results['MNIST'] = {'Accuracy': acc_mnist, 'Precision': prec_mnist, 'Recall': rec_mnist, 'F1-score': f1_mnist}\n",
        "\n",
        "# Train and evaluate on Fashion MNIST dataset\n",
        "acc_fmnist, prec_fmnist, rec_fmnist, f1_fmnist = train_lenet5('Fashion MNIST', x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist)\n",
        "results['Fashion MNIST'] = {'Accuracy': acc_fmnist, 'Precision': prec_fmnist, 'Recall': rec_fmnist, 'F1-score': f1_fmnist}\n",
        "\n",
        "# Train and evaluate on CIFAR-10 dataset\n",
        "acc_cifar, prec_cifar, rec_cifar, f1_cifar = train_lenet5('CIFAR-10', x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)\n",
        "results['CIFAR-10'] = {'Accuracy': acc_cifar, 'Precision': prec_cifar, 'Recall': rec_cifar, 'F1-score': f1_cifar}\n",
        "\n",
        "# Print results\n",
        "print(\"\\nResults:\")\n",
        "for dataset, metrics in results.items():\n",
        "    print(f\"{dataset}:\")\n",
        "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['Precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['Recall']:.4f}\")\n",
        "    print(f\"  F1-score: {metrics['F1-score']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Write results to a text file\n",
        "with open('LeNet-5.txt', 'w') as f:\n",
        "    f.write(\"Final Results:\\n\")\n",
        "    for dataset, metrics in results.items():\n",
        "        f.write(f\"\\n{dataset} dataset:\\n\")\n",
        "        for metric, value in metrics.items():\n",
        "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
        "\n",
        "# Comparison of metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results.keys(), [metrics['Accuracy'] for metrics in results.values()], label='Accuracy')\n",
        "plt.bar(results.keys(), [metrics['Precision'] for metrics in results.values()], label='Precision')\n",
        "plt.bar(results.keys(), [metrics['Recall'] for metrics in results.values()], label='Recall')\n",
        "plt.bar(results.keys(), [metrics['F1-score'] for metrics in results.values()], label='F1-score')\n",
        "plt.title('Comparison of Performance Metrics - LeNet-5')\n",
        "plt.xlabel('Datasets')\n",
        "plt.ylabel('Metrics')\n",
        "plt.ylim(0, 1.2)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nScript execution completed.\")"
      ],
      "metadata": {
        "id": "UhIevfi9I5BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet.py**"
      ],
      "metadata": {
        "id": "8C5yzvAzJCev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "# Function to build a simple ResNet model\n",
        "def build_resnet(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Residual block\n",
        "    def res_block(x, filters):\n",
        "        y = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
        "        y = layers.Conv2D(filters, (3, 3), padding='same', activation=None)(y)\n",
        "        if x.shape[-1] != filters:\n",
        "            x = layers.Conv2D(filters, (1, 1), padding='same', activation=None)(x)\n",
        "        return layers.add([x, y])\n",
        "\n",
        "    x = res_block(x, 64)\n",
        "    x = res_block(x, 128)\n",
        "    x = res_block(x, 256)\n",
        "    x = res_block(x, 512)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess and load MNIST data\n",
        "def load_and_preprocess_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_train, _, y_train, _ = train_test_split(x_train, y_train, train_size=0.75, random_state=42)\n",
        "\n",
        "    x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
        "    x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to preprocess and load Fashion MNIST data\n",
        "def load_and_preprocess_fashion_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    x_train, _, y_train, _ = train_test_split(x_train, y_train, train_size=0.75, random_state=42)\n",
        "\n",
        "    x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
        "    x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to preprocess and load CIFAR-10 data\n",
        "def load_and_preprocess_cifar10():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to train and evaluate the ResNet model\n",
        "def train_resnet(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    model = build_resnet(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128, validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "    scores = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{dataset_name} ResNet Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{dataset_name} ResNet Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save plots as PNG\n",
        "    plt.savefig(f\"{dataset_name.lower()}_resnet_training_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'test_accuracy': scores[1],\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Load and preprocess data for each dataset\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_and_preprocess_mnist()\n",
        "x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist = load_and_preprocess_fashion_mnist()\n",
        "x_train_cifar10, y_train_cifar10, x_test_cifar10, y_test_cifar10 = load_and_preprocess_cifar10()\n",
        "\n",
        "# Train and evaluate ResNet model on each dataset\n",
        "mnist_scores = train_resnet('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "fashion_mnist_scores = train_resnet('Fashion MNIST', x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "cifar10_scores = train_resnet('CIFAR-10', x_train_cifar10, y_train_cifar10, x_test_cifar10, y_test_cifar10)\n",
        "\n",
        "# Save the scores\n",
        "all_scores = {\n",
        "    'MNIST': mnist_scores,\n",
        "    'Fashion MNIST': fashion_mnist_scores,\n",
        "    'CIFAR-10': cifar10_scores\n",
        "}\n",
        "\n",
        "with open('resnet_scores.json', 'w') as f:\n",
        "    json.dump(all_scores, f, indent=4)\n",
        "\n",
        "# Save the scores to a text file\n",
        "with open('resnet_scores.txt', 'w') as f:\n",
        "    for dataset, scores in all_scores.items():\n",
        "        f.write(f\"{dataset} Scores:\\n\")\n",
        "        for metric, value in scores.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "4bajjV4EJIHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SeNet.py**"
      ],
      "metadata": {
        "id": "xYYLiC5FJSSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# Enable mixed precision - updated import\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy) # Use tf.keras to set policy\n",
        "\n",
        "# Load datasets\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "(x_train_fashion_mnist, y_train_fashion_mnist), (x_test_fashion_mnist, y_test_fashion_mnist) = fashion_mnist.load_data()\n",
        "(x_train_cifar10, y_train_cifar10), (x_test_cifar10, y_test_cifar10) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to between 0 and 1\n",
        "x_train_mnist = x_train_mnist.astype('float32') / 255.0\n",
        "x_test_mnist = x_test_mnist.astype('float32') / 255.0\n",
        "x_train_fashion_mnist = x_train_fashion_mnist.astype('float32') / 255.0\n",
        "x_test_fashion_mnist = x_test_fashion_mnist.astype('float32') / 255.0\n",
        "x_train_cifar10 = x_train_cifar10.astype('float32') / 255.0\n",
        "x_test_cifar10 = x_test_cifar10.astype('float32') / 255.0\n",
        "\n",
        "# Adjust input shape for different datasets\n",
        "x_train_mnist = np.expand_dims(x_train_mnist, axis=-1)  # Add channel dimension for MNIST\n",
        "x_test_mnist = np.expand_dims(x_test_mnist, axis=-1)\n",
        "x_train_fashion_mnist = np.expand_dims(x_train_fashion_mnist, axis=-1)  # Add channel dimension for Fashion MNIST\n",
        "x_test_fashion_mnist = np.expand_dims(x_test_fashion_mnist, axis=-1)\n",
        "# CIFAR-10 already has 3 channels (RGB)\n",
        "input_shape_cifar10 = x_train_cifar10.shape[1:]\n",
        "\n",
        "# Function to create SENet model\n",
        "def build_senet(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Squeeze and Excitation Block\n",
        "    squeeze = layers.GlobalAveragePooling2D()(x)\n",
        "    excitation = layers.Dense(64, activation='relu')(squeeze)\n",
        "    excitation = layers.Dense(128, activation='sigmoid')(excitation)\n",
        "    excitation = layers.Reshape((1, 1, 128))(excitation)\n",
        "    x = layers.multiply([x, excitation])\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function to train and evaluate SENet model\n",
        "def train_senet(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    if dataset_name == 'CIFAR-10':\n",
        "        input_shape = input_shape_cifar10\n",
        "    else:\n",
        "        input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    model = build_senet(input_shape, num_classes)\n",
        "    history = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128, validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"{dataset_name} - Test Loss: {scores[0]}, Test Accuracy: {scores[1]}\")\n",
        "    print(f\"{dataset_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "\n",
        "    # Plot loss and accuracy curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='train_loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.title(f'{dataset_name} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.title(f'{dataset_name} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'test_loss': scores[0],\n",
        "        'test_accuracy': scores[1],\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Train and evaluate SENet model on each dataset\n",
        "mnist_scores = train_senet('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "fashion_mnist_scores = train_senet('Fashion MNIST', x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "cifar10_scores = train_senet('CIFAR-10', x_train_cifar10, y_train_cifar10, x_test_cifar10, y_test_cifar10)\n",
        "\n",
        "# Save the scores to a JSON file\n",
        "all_scores = {\n",
        "    'MNIST': mnist_scores,\n",
        "    'Fashion MNIST': fashion_mnist_scores,\n",
        "    'CIFAR-10': cifar10_scores\n",
        "}\n",
        "\n",
        "with open('senet_scores.json', 'w') as f:\n",
        "    json.dump(all_scores, f, indent=4)\n",
        "\n",
        "# Save the scores to a text file\n",
        "with open('senet_scores.txt', 'w') as f:\n",
        "    for dataset, scores in all_scores.items():\n",
        "        f.write(f\"{dataset} Scores:\\n\")\n",
        "        for metric, value in scores.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "lx0xLf3mJr90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VGGnet.py**"
      ],
      "metadata": {
        "id": "6D82pwMIJ0Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Function to build VGGNet model\n",
        "def build_vggnet(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Block 2\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Block 3\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Block 4\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Block 5\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to preprocess and load MNIST data\n",
        "def load_and_preprocess_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
        "    x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to preprocess and load Fashion MNIST data\n",
        "def load_and_preprocess_fashion_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
        "    x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "    x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "    x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to preprocess and load CIFAR-10 data\n",
        "def load_and_preprocess_cifar10():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Function to train VGGNet on a dataset\n",
        "def train_vggnet(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    model = build_vggnet(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(f\"Training VGGNet on {dataset_name} dataset...\")\n",
        "    history = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128, validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "    print(f\"Evaluating VGGNet on {dataset_name} dataset...\")\n",
        "    test_loss, test_acc = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "    precision = precision_score(y_test, y_pred_class, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_class, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
        "\n",
        "    metrics_text = f\"Dataset: {dataset_name}\\n\" \\\n",
        "                   f\"Test accuracy: {test_acc:.4f}\\n\" \\\n",
        "                   f\"Precision: {precision:.4f}\\n\" \\\n",
        "                   f\"Recall: {recall:.4f}\\n\" \\\n",
        "                   f\"F1-score: {f1:.4f}\\n\"\n",
        "\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "    with open(f\"metrics/{dataset_name}_vggnet_metrics.txt\", 'w') as f:\n",
        "        f.write(metrics_text)\n",
        "\n",
        "    print(metrics_text)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{dataset_name} Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{dataset_name} Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "    plt.savefig(f\"plots/{dataset_name}_vggnet_training_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Load and preprocess datasets\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_and_preprocess_mnist()\n",
        "x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist = load_and_preprocess_fashion_mnist()\n",
        "x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar = load_and_preprocess_cifar10()\n",
        "\n",
        "# Train and evaluate VGGNet on MNIST\n",
        "train_vggnet('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "\n",
        "# Train and evaluate VGGNet on Fashion MNIST\n",
        "train_vggnet('Fashion_MNIST', x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "\n",
        "# Train and evaluate VGGNet on CIFAR-10\n",
        "train_vggnet('CIFAR-10', x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)\n",
        "\n",
        "\n",
        "# Save the scores to a JSON file\n",
        "all_scores = {\n",
        "    'MNIST': mnist_scores,\n",
        "    'Fashion MNIST': fashion_mnist_scores,\n",
        "    'CIFAR-10': cifar10_scores\n",
        "}\n",
        "\n",
        "with open('vggnet_scores.json', 'w') as f:\n",
        "    json.dump(all_scores, f, indent=4)\n",
        "\n",
        "# Save the scores to a text file\n",
        "with open('vggnet_scores.txt', 'w') as f:\n",
        "    for dataset, scores in all_scores.items():\n",
        "        f.write(f\"{dataset} Scores:\\n\")\n",
        "        for metric, value in scores.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GQTIraysJ2h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Xception.py**"
      ],
      "metadata": {
        "id": "0VtdcOMuJ9_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.mixed_precision import Policy\n"
      ],
      "metadata": {
        "id": "8DNSTOWjfBjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n"
      ],
      "metadata": {
        "id": "SG8-qVx2fCit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "id": "Iwkoq-x9fCGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6wocQ3TGJn5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import json\n",
        "\n",
        "# Enable mixed precision\n",
        "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
        "policy = Policy('mixed_float16')\n",
        "set_global_policy(policy)\n",
        "\n",
        "# Import the datasets\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_images(x):\n",
        "    x = tf.image.resize(x, (71, 71))  # Resize to 71x71\n",
        "    x = x / 255.0  # Normalize\n",
        "    return x\n",
        "\n",
        "# Function to create a smaller CNN model (MobileNetV2)\n",
        "def build_mobilenet(input_shape, num_classes):\n",
        "    base_model = tf.keras.applications.MobileNetV2(weights=None, include_top=False, input_shape=input_shape)\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=base_model.input, outputs=x)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function to train and evaluate the model using generators\n",
        "def train_model_with_generator(dataset_name, x_train, y_train, x_test, y_test, batch_size=32, epochs=5):\n",
        "    input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    y_train_cat = to_categorical(y_train, num_classes)\n",
        "    y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Use ImageDataGenerator to generate data in batches to save memory\n",
        "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_images)\n",
        "    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_images)\n",
        "\n",
        "    # Create generators\n",
        "    train_generator = train_datagen.flow(x_train, y_train_cat, batch_size=batch_size)\n",
        "    test_generator = test_datagen.flow(x_test, y_test_cat, batch_size=batch_size)\n",
        "\n",
        "    # Build the model\n",
        "    model = build_mobilenet(input_shape, num_classes)\n",
        "\n",
        "    # Train the model using the data generators\n",
        "    history = model.fit(train_generator, epochs=epochs, validation_data=test_generator, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(test_generator, verbose=0)\n",
        "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"{dataset_name} - Test Loss: {scores[0]}, Test Accuracy: {scores[1]}\")\n",
        "    print(f\"{dataset_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "\n",
        "    # Plot loss and accuracy curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='train_loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.title(f'{dataset_name} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.title(f'{dataset_name} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Clear Keras session to release memory after training\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Explicit garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'test_loss': scores[0],\n",
        "        'test_accuracy': scores[1],\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "(x_train_fashion_mnist, y_train_fashion_mnist), (x_test_fashion_mnist, y_test_fashion_mnist) = fashion_mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Resize and preprocess datasets\n",
        "x_train_mnist = preprocess_images(np.expand_dims(x_train_mnist, axis=-1))\n",
        "x_test_mnist = preprocess_images(np.expand_dims(x_test_mnist, axis=-1))\n",
        "x_train_fashion_mnist = preprocess_images(np.expand_dims(x_train_fashion_mnist, axis=-1))\n",
        "x_test_fashion_mnist = preprocess_images(np.expand_dims(x_test_fashion_mnist, axis=-1))\n",
        "x_train_cifar = preprocess_images(x_train)\n",
        "x_test_cifar = preprocess_images(x_test)\n",
        "\n",
        "# Use only a subset of the data for faster experiments (1000 samples)\n",
        "x_train_mnist, y_train_mnist = x_train_mnist[:1000], y_train_mnist[:1000]\n",
        "x_train_fashion_mnist, y_train_fashion_mnist = x_train_fashion_mnist[:1000], y_train_fashion_mnist[:1000]\n",
        "\n",
        "# Train and evaluate the model on MNIST, Fashion MNIST, and CIFAR-10 datasets\n",
        "mnist_scores = train_model_with_generator('MNIST', x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)\n",
        "fashion_mnist_scores = train_model_with_generator('Fashion MNIST', x_train_fashion_mnist, y_train_fashion_mnist, x_test_fashion_mnist, y_test_fashion_mnist)\n",
        "cifar10_scores = train_model_with_generator('Cifar-10', x_train_cifar, y_train, x_test_cifar, y_test)\n",
        "\n",
        "# Save the scores to a JSON file\n",
        "all_scores = {\n",
        "    'MNIST': mnist_scores,\n",
        "    'Fashion MNIST': fashion_mnist_scores,\n",
        "    'Cifar-10': cifar10_scores\n",
        "}\n",
        "\n",
        "# Save the scores to a text file\n",
        "with open('mobilenet_scores.txt', 'w') as f:\n",
        "    for dataset, scores in all_scores.items():\n",
        "        f.write(f\"{dataset} Scores:\\n\")\n",
        "        for metric, value in scores.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "qxDrJSnO6hwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **googlenet.py**"
      ],
      "metadata": {
        "id": "J10-aJU-KF_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to build GoogLeNet (Inception v1) model\n",
        "def build_googlenet(input_shape, num_classes):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    conv1_7x7_s2 = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu', name='conv1_7x7_s2')(input_layer)\n",
        "    maxpool1_3x3_s2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='maxpool1_3x3_s2')(conv1_7x7_s2)\n",
        "\n",
        "    conv2_3x3_reduce = layers.Conv2D(64, (1, 1), padding='same', activation='relu', name='conv2_3x3_reduce')(maxpool1_3x3_s2)\n",
        "    conv2_3x3 = layers.Conv2D(192, (3, 3), padding='same', activation='relu', name='conv2_3x3')(conv2_3x3_reduce)\n",
        "    maxpool2_3x3_s2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='maxpool2_3x3_s2')(conv2_3x3)\n",
        "\n",
        "    inception_3a = inception_module(maxpool2_3x3_s2, 64, 96, 128, 16, 32, 32)\n",
        "    inception_3b = inception_module(inception_3a, 128, 128, 192, 32, 96, 64)\n",
        "    maxpool3_3x3_s2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='maxpool3_3x3_s2')(inception_3b)\n",
        "\n",
        "    inception_4a = inception_module(maxpool3_3x3_s2, 192, 96, 208, 16, 48, 64)\n",
        "    inception_4b = inception_module(inception_4a, 160, 112, 224, 24, 64, 64)\n",
        "    inception_4c = inception_module(inception_4b, 128, 128, 256, 24, 64, 64)\n",
        "    inception_4d = inception_module(inception_4c, 112, 144, 288, 32, 64, 64)\n",
        "    inception_4e = inception_module(inception_4d, 256, 160, 320, 32, 128, 128)\n",
        "    maxpool4_3x3_s2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='maxpool4_3x3_s2')(inception_4e)\n",
        "\n",
        "    inception_5a = inception_module(maxpool4_3x3_s2, 256, 160, 320, 32, 128, 128)\n",
        "    inception_5b = inception_module(inception_5a, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "    # Adjust avgpool pool_size to be compatible with different input sizes\n",
        "    avgpool = layers.AveragePooling2D(pool_size=(1, 1), strides=(1, 1), padding='valid', name='avgpool')(inception_5b)\n",
        "\n",
        "    drop = layers.Dropout(0.4)(avgpool)\n",
        "\n",
        "    flat = layers.Flatten()(drop)\n",
        "    output = layers.Dense(num_classes, activation='softmax', name='output')(flat)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output, name='inception_v1')\n",
        "\n",
        "    return model\n",
        "\n",
        "def inception_module(prev_layer, conv1_filters, conv3_reduce_filters, conv3_filters, conv5_reduce_filters, conv5_filters, pool_filters):\n",
        "    conv1x1 = layers.Conv2D(conv1_filters, (1, 1), padding='same', activation='relu')(prev_layer)\n",
        "\n",
        "    conv3x3_reduce = layers.Conv2D(conv3_reduce_filters, (1, 1), padding='same', activation='relu')(prev_layer)\n",
        "    conv3x3 = layers.Conv2D(conv3_filters, (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n",
        "\n",
        "    conv5x5_reduce = layers.Conv2D(conv5_reduce_filters, (1, 1), padding='same', activation='relu')(prev_layer)\n",
        "    conv5x5 = layers.Conv2D(conv5_filters, (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n",
        "\n",
        "    pool_proj = layers.Conv2D(pool_filters, (1, 1), padding='same', activation='relu')(layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(prev_layer))\n",
        "\n",
        "    return layers.concatenate([conv1x1, conv3x3, conv5x5, pool_proj])\n",
        "\n",
        "# Function to preprocess and load data\n",
        "def load_and_preprocess_data():\n",
        "    from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "\n",
        "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "    (x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
        "    (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "\n",
        "    # Normalize data\n",
        "    x_train_mnist = x_train_mnist.astype('float32') / 255.0\n",
        "    x_test_mnist = x_test_mnist.astype('float32') / 255.0\n",
        "    x_train_fmnist = x_train_fmnist.astype('float32') / 255.0\n",
        "    x_test_fmnist = x_test_fmnist.astype('float32') / 255.0\n",
        "    x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "    x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "    # Reshape for CNN input\n",
        "    x_train_mnist = np.expand_dims(x_train_mnist, axis=-1)\n",
        "    x_test_mnist = np.expand_dims(x_test_mnist, axis=-1)\n",
        "    x_train_fmnist = np.expand_dims(x_train_fmnist, axis=-1)\n",
        "    x_test_fmnist = np.expand_dims(x_test_fmnist, axis=-1)\n",
        "\n",
        "    # CIFAR-10 needs 3 color channels\n",
        "    x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "    x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "    return (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist), \\\n",
        "           (x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist), \\\n",
        "           (x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar)\n",
        "\n",
        "# Function to train GoogLeNet on a dataset\n",
        "def train_googlenet(dataset_name, x_train, y_train, x_test, y_test):\n",
        "    input_shape = x_train.shape[1:]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    # Build GoogLeNet model\n",
        "    model = build_googlenet(input_shape, num_classes)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training GoogLeNet on {dataset_name} dataset...\")\n",
        "    history = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128, validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Evaluating GoogLeNet on {dataset_name} dataset...\")\n",
        "    test_loss, test_acc = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "    precision = precision_score(y_test, y_pred_class, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_class, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
        "\n",
        "    # Print and save evaluation metrics to a text file\n",
        "    metrics_text = f\"Dataset: {dataset_name}\\n\" \\\n",
        "                   f\"Test accuracy: {test_acc:.4f}\\n\" \\\n",
        "                   f\"Precision: {precision:.4f}\\n\" \\\n",
        "                   f\"Recall: {recall:.4f}\\n\" \\\n",
        "                   f\"F1 score: {f1:.4f}\\n\"\n",
        "\n",
        "    print(metrics_text)\n",
        "    with open(f\"metrics_{dataset_name}.txt\", \"w\") as f:\n",
        "        f.write(metrics_text)\n",
        "\n",
        "    # Plot training and validation accuracy/loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "    plt.title(f'{dataset_name} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='train loss')\n",
        "    plt.plot(history.history['val_loss'], label='val loss')\n",
        "    plt.title(f'{dataset_name} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Load datasets and preprocess them\n",
        "mnist_data, fmnist_data, cifar10_data = load_and_preprocess_data()\n",
        "\n",
        "# Train and evaluate GoogLeNet on MNIST\n",
        "train_googlenet(\"MNIST\", *mnist_data)\n",
        "\n",
        "# Train and evaluate GoogLeNet on FMNIST\n",
        "train_googlenet(\"FMNIST\", *fmnist_data)\n",
        "\n",
        "# Train and evaluate GoogLeNet on CIFAR-10\n",
        "train_googlenet(\"CIFAR-10\", *cifar10_data)\n",
        "\n",
        "\n",
        "# Save the scores to a JSON file\n",
        "all_scores = {\n",
        "    'MNIST': mnist_scores,\n",
        "    'Fashion MNIST': fashion_mnist_scores,\n",
        "    'CIFAR-10': cifar10_scores\n",
        "}\n",
        "\n",
        "with open('googlenet_scores.json', 'w') as f:\n",
        "    json.dump(all_scores, f, indent=4)\n",
        "\n",
        "# Save the scores to a text file\n",
        "with open('googlenet_scores.txt', 'w') as f:\n",
        "    for dataset, scores in all_scores.items():\n",
        "        f.write(f\"{dataset} Scores:\\n\")\n",
        "        for metric, value in scores.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "u5rynEfu9a_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}